{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFYpvF5NYJDE"
   },
   "source": [
    "# Text to Speech\n",
    "\n",
    "When solving some problem with deep learning in practice, you search the Web for the latest paper that solves that task, and take its implementation from GitHub. However, often there is no code, so being able to **reimplement a paper** is a vital skill. You will likely have to do it in your course project, and we are going to practice it in this assignment.\n",
    "\n",
    "Let's focus on the task of text-to-speech (**TTS**) synthesis.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/9570420/81783573-392ed600-9504-11ea-98da-86ac05457c29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDkpVP7L17Cs"
   },
   "source": [
    "## Data\n",
    "\n",
    " Intuitively, in the real world your dataset would be a set of pairs:\n",
    "\n",
    "* text (string);\n",
    "* **target**: raw audio of a person saying `text` (array of amplitude values sampled e.g. 44100 times per second â€” see an example plotted above).\n",
    "\n",
    "We give you a simplified problem statement, with one dataset sample being an utterance described by\n",
    "\n",
    "* list of [ARPAbet phonemes](http://www.speech.cs.cmu.edu/cgi-bin/cmudict#phones);\n",
    "* *phoneme alignment*, i.e. start time (frame) and duration for each phoneme;\n",
    "* **target**: [mel spectrogram](https://pytorch.org/audio/transforms.html#torchaudio.transforms.MelSpectrogram) of a person saying text.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/9570420/81795777-2a9cea80-9515-11ea-99eb-05915f803af1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6j4OMeTBv7oJ"
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AzoGI4xleZB"
   },
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2020 -- Home Assignment 4\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgotten to 'Add a shortcut to Drive'?\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.insert(0, str(AUX_DATA_ROOT))\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8odGWRGmDpV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "![ ! -d \"LJ-Speech-aligned\" ] && unzip -q \"{AUX_DATA_ROOT / 'LJ-Speech-aligned.zip'}\"\n",
    "!git clone --recursive https://github.com/shrubb/waveglow.git -b denoiser-fix\n",
    "!wget -c https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ljs_256channels/versions/2/files/waveglow_256channels_ljs_v2.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlknjzQxv7ok"
   },
   "source": [
    "Use this Python module to handle our dataset. It's documented, so when in doubt, use `help()` or read the code with `??lj_speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1pfMR2Bv7ok"
   },
   "outputs": [],
   "source": [
    "import lj_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwejikNwlUXv"
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('LJ-Speech-aligned/')\n",
    "\n",
    "train_dataset, val_dataset = lj_speech.get_dataset(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIOsxKpxv7ox"
   },
   "source": [
    "There are also a couple of useful constants, check them with `?lj_speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gG6C87q4v7oz"
   },
   "outputs": [],
   "source": [
    "len(lj_speech.POSSIBLE_PHONEME_CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHv9pTNZv7o4"
   },
   "source": [
    "Here is an example datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxjI6_4PlUYz"
   },
   "outputs": [],
   "source": [
    "example_datapoint = train_dataset[666]\n",
    "print(f\"Datasets yield: {', '.join(example_datapoint.keys())}\")\n",
    "plt.imshow(example_datapoint['spectrogram']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaNwF1j1v7o-"
   },
   "source": [
    "To \"play\" spectrograms, including those that you will generate, we will use another deep learning algorithm called [WaveGlow](https://arxiv.org/abs/1811.00002). It converts mel spectrograms to audio.\n",
    "\n",
    "Fortunately, there *is* code for it on GitHub, so you won't have to reimplement it ðŸ™‚ Still, if you haven't done so, I encourage you to watch a short [video](https://www.youtube.com/watch?v=CqFIVCD1WWo) about a famous paper that it's based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Yl1eXTbv7o-"
   },
   "outputs": [],
   "source": [
    "vocoder = lj_speech.Vocoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCVdjPzDv7pG"
   },
   "outputs": [],
   "source": [
    "print(example_datapoint['text'])\n",
    "\n",
    "example_spectrogram = torch.from_numpy(example_datapoint['spectrogram'])\n",
    "audio = vocoder(example_spectrogram)\n",
    "lj_speech.play_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5cG-sPVKwie"
   },
   "source": [
    "Finally, we have phonemes as inputs, but we'd like to synthesize arbitrary text. For that, there is a function `lj_speech.text_to_phonemes(text)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV22STdav7pK"
   },
   "source": [
    "## Method\n",
    "\n",
    "Reimplement the baseline method defined by the diagram below. It's derived from a simplified version of the [DurIAN paper](https://arxiv.org/abs/1909.01700).\n",
    "\n",
    "Just in case, here are some of the most notable differences from DurIAN:\n",
    "* Prosodic boundary markers aren't used (we don't have them labeled), and thus there's no 'skip states' exclusion of prosodic boundaries' hidden states.\n",
    "* Style codes aren't used too (same).\n",
    "* Simpler network architectures.\n",
    "* No pre-net in decoder.\n",
    "* No attention used in decoder.\n",
    "* Decoder's recurrent cell outputs single spectrogram frame at a time.\n",
    "* Decoder's recurrent cell isn't conditioned on its own outputs (isn't \"autogressive\").\n",
    "\n",
    "![pipeline](https://user-images.githubusercontent.com/9570420/81863803-6f0ba300-9574-11ea-9f02-481c2bba81f0.png)\n",
    "\n",
    "This picture is the simplified Figure 1 from the paper. **Use the paper as a reference**. If something is unclear from the diagram â€” and, in fact, some things are intentionally omitted â€” search answers in the paper (but remain aware of the differences/simplifications in the above diagram) and other papers that the authors cite. There will even be details that aren't even mentioned anywhere in the paper, so you'll have to guess by your intuition. For instance, DurIAN paper doesn't explain batch size and sampling strategy, and even if it did, it would be irrelevant since we use different data. Finally, search the Web and read others' code; however, if you copy-paste code, **cite it**.\n",
    "\n",
    "Tips:\n",
    "* Only compute loss on the 'original' parts of the spectrograms (don't include padding)!\n",
    "* When using recurrent nets, [clip gradients' norm](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_).\n",
    "* Since you will use recurrent nets, and the sequences in a batch will be of different lengths, you may find [sequence packing utility](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) useful.\n",
    "* Remember that you can send images and audio into TensorBoard, too.\n",
    "\n",
    "**Train this baseline so that the words are well-recognizable, and try to get rid of the \"metallic\", \"tin sounding\" voice.**\n",
    "\n",
    "**Aim for something like this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi6god6fv7pM"
   },
   "outputs": [],
   "source": [
    "lj_speech.play_audio(scipy.io.wavfile.read(AUX_DATA_ROOT / 'ok.wav')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVDxbQ4Fv7pR"
   },
   "source": [
    "**On the other hand, here is an example of insufficient quality:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uugubYIdv7pT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lj_speech.play_audio(scipy.io.wavfile.read(AUX_DATA_ROOT / 'bad.wav')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjG9g1qhv7pZ"
   },
   "source": [
    "## Improving the Method\n",
    "\n",
    "When you have the baseline working, seek the ways to improve quality and to decrease loss. Look into the paper and the relevant/neighbouring literature. Also, follow your intuition, e.g. for changing model complexity or regularization. For example, you can (but aren't obliged to) try borrowing more tricks from DurIAN; a couple of examples for inspiration:\n",
    "\n",
    "* Make decoder recurrent cell autoregressive as in the paper: use its output as its input at the next preiction step.\n",
    "* Add pre-net to decoder.\n",
    "* Add attention to decoder.\n",
    "* Use more complex sub-network architectures (e.g. use CBHG).\n",
    "* Employ tricks from Tacotron {[1](https://arxiv.org/abs/1703.10135),[2](https://arxiv.org/abs/1712.05884)} â€” papers that DurIAN is based upon, e.g. [zoneout](https://arxiv.org/abs/1606.01305) in RNNs.\n",
    "\n",
    "Write a report on your journey to better loss. Explain if you managed to perceptibly improve the quality and lower the loss, what you tried and why, what worked and what didn't. Include TensorBoard loss plots.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. This file, fully functional, reproducing all the reported results on \"Run all\". Out-of-the box `TextToSpeechSynthesizer` class (see interface below) and demo code of running it on arbitrary text.\n",
    "2. Report on improving the baseline at the end of this file.\n",
    "3. Two \"checkpoint files\" with weights for both of your models (baseline and the improved model).\n",
    "\n",
    "## Grading\n",
    "\n",
    "* **[7 points]** Baseline.\n",
    "* **[3 points]** Improving of the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zetnqJQ77AKr"
   },
   "outputs": [],
   "source": [
    "class TextToSpeechSynthesizer:\n",
    "    \"\"\"\n",
    "    When done, please fill this class. It should work out-of-the-box, i.e.\n",
    "    have a simple interface, automatically load model weights,\n",
    "    process text to speech at one command without errors etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path, <any-other-arguments>):\n",
    "        \"\"\"\n",
    "        Initialize anything you may need. For example, load model weights from disk.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def synthesize_from_text(self, text):\n",
    "        \"\"\"\n",
    "        Synthesize text into voice.\n",
    "\n",
    "        text:\n",
    "            str\n",
    "        \n",
    "        return:\n",
    "        audio:\n",
    "            torch.Tensor or numpy.ndarray, shape == (1, t)\n",
    "        \"\"\"\n",
    "        phonemes = lj_speech.text_to_phonemes(text)\n",
    "        return self.synthesize_from_phonemes(phonemes)\n",
    "\n",
    "    def synthesize_from_phonemes(self, phonemes, durations=None):\n",
    "        \"\"\"\n",
    "        Synthesize phonemes into voice.\n",
    "\n",
    "        phonemes:\n",
    "            list of str\n",
    "            ARPAbet phoneme codes.\n",
    "        durations:\n",
    "            list of int, optional\n",
    "            Duration in spectrogram frames for each phoneme.\n",
    "            If given, will be used for hard alignment in the model (like during\n",
    "            training); otherwise, durations will be predicted by the duration\n",
    "            model.\n",
    "        \n",
    "        return:\n",
    "        audio:\n",
    "            torch.Tensor or numpy.ndarray, shape == (1, t)\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "\n",
    "        spectrogram = ...\n",
    "        return vocoder(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4_XnykuOLuP"
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "synthesizer = TextToSpeechSynthesizer(\"./baseline.pth\", <any-other-arguments>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiNIB463MkbA"
   },
   "outputs": [],
   "source": [
    "phonemes = \"DH IH1 S pau pau pau IH1 Z pau pau pau S P AH1 R T AH1 AH1 AH1 pau\".split()\n",
    "lj_speech.play_audio(\n",
    "    synthesizer.synthesize_from_phonemes(phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cscryy2PNg4w"
   },
   "outputs": [],
   "source": [
    "text = \"Pack my box with five dozen liquor jugs.\"\n",
    "# text = \"The five boxing wizards jump quickly.\"\n",
    "# text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# text = \"How about some school tech.\"\n",
    "# text = \"Last homework. We are in a deep trouble. No sleep tonight.\"\n",
    "\n",
    "lj_speech.play_audio(\n",
    "    synthesizer.synthesize_from_text(text))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
